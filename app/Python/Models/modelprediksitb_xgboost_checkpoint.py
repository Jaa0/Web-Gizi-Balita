# -*- coding: utf-8 -*-
"""ModelPrediksiTB_XGBoost-checkpoint.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1frC6EHtb_yUuNVA83BRBJPBM5_w-9Rg4
"""

# Importing Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.model_selection import cross_val_score, RandomizedSearchCV
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

# Importing Dataset
df = pd.read_excel('EN071R_REGISTER POLI IMUN SDIDTK JAN - 18 SEPTEMBER.xls')

"""# Data Understanding & Cleaning"""

df.head()

# Drop unnecessary column
df = df[['Umur', 'Jenis Kelamin', 'TB', 'INDEX PB/U']]
df

df.info()

df.describe()

# Handling null values
df.isna().sum()

df = df.dropna()
df = df.reset_index(drop=True)
df.info()

plt.figure(figsize=(15, 10))
sns.countplot(x='INDEX PB/U', data=df)
plt.title('Count of Each Classes', fontsize=15)
plt.xlabel('INDEX PB/U', fontsize=18)
plt.ylabel('Count', fontsize=18)
plt.show

df.rename(columns={'Umur': 'Age in Month', 'TB': 'Height', 'Jenis Kelamin': 'Gender'}, inplace=True)
df

# Convert Age Into Month
def convert_to_months(Age):
    # Separate the string
    parts = Age.split()
    year = int(parts[0])
    month = int(parts[2])
    # Convert and add months
    age_in_month = (year * 12) + month
    return age_in_month

df['Age in Month'] = df['Age in Month'].apply(convert_to_months)
df

# Convert Age in Month to Integer
df['Age in Month'] = df['Age in Month'].astype(int)
df.info()

# Remove rows with Age in Month above 60
df = df[df['Age in Month'] <= 60]

# Reset index after filtering
df = df.reset_index(drop=True)

print(df['INDEX PB/U'].value_counts())

"""# Data Pre-Processing"""

# Encode categorical column
df['Gender'] = df['Gender'].map({'PRIA':0, 'WANITA':1})
df['INDEX PB/U'] = df['INDEX PB/U'].map({'NORMAL':0, 'PENDEK':1, 'SANGAT PENDEK':2, 'TINGGI':3})
print(df['Gender'].unique())
print(df['INDEX PB/U'].unique())

# Split training and testing data
X = df.drop(["INDEX PB/U"], axis=1)
y = df["INDEX PB/U"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify=y)

print(f'Total # of Sample in Whole Dataset: {len(X)}')
print(f'Total # of Sample in Train Dataset: {len(X_train)}')
print(f'Total # of Sample in Test Dataset: {len(X_test)}')

# K-fold
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Normalize numerical features
numerical_features = ['Age in Month', 'Height']
scaler = StandardScaler()

# Fit the scaler on the training data and transform it
X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])

# Transform the test data using the same scaler
X_test[numerical_features] = scaler.transform(X_test[numerical_features])
X_train[numerical_features].head()

"""# Model Development"""

# initialize Random Classifier
xg = XGBClassifier()

# define the parameter grid
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0]
}

# Specify multiple scoring metrics
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score, average='weighted'),
    'recall': make_scorer(recall_score, average='weighted'),
    'f1': make_scorer(f1_score, average='weighted')
}

random_search = RandomizedSearchCV(estimator=xg,
                                   param_distributions=param_dist,
                                   n_iter=50,  # Number of combinations to sample
                                   scoring=scoring,
                                   refit='f1',
                                   cv=kfold,
                                   n_jobs=-1,
                                   verbose=1,
                                   random_state=42)  # For reproducibility

# Fit the model
random_search.fit(X_train, y_train)

# Retrieve the best model
best_xgboost_model_tb = random_search.best_estimator_

# Export the best model using joblib
joblib.dump(best_xgboost_model_tb, 'best_xgboost_model_tb.joblib')

# Evaluate the model on the test data
y_pred = best_xgboost_model_tb.predict(X_test)
y_prob = best_xgboost_model_tb.predict_proba(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['NORMAL', 'PENDEK', 'SANGAT PENDEK', 'TINGGI'])
fig, ax = plt.subplots(figsize=(20, 10))
disp.plot(cmap=plt.cm.Blues, ax=ax)
plt.title("Confusion Matrix", fontsize=18)
plt.ylabel("Actual Label", fontsize=15)
plt.xlabel("Predicted Label", fontsize=15)
plt.yticks(fontsize=18)
plt.xticks(fontsize=18)
plt.show()

# Classification evaluation scores (Accuracy, Precision, Recall, F1)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

from sklearn.preprocessing import label_binarize

n_classes = len(np.unique(y_test))

# Binarize the labels for multiclass
y_test_binarized = label_binarize(y_test, classes=np.arange(n_classes))

# Initialize dictionaries for FPR, TPR, and AUC
fpr = dict()
tpr = dict()
roc_auc = dict()

# Compute ROC curve and ROC AUC for each class
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])
    roc_auc[i] = roc_auc_score(y_test_binarized[:, i], y_prob[:, i])

# Compute micro-average ROC curve and ROC AUC
fpr_micro, tpr_micro, _ = roc_curve(y_test_binarized.ravel(), y_prob.ravel())
roc_auc_micro = roc_auc_score(y_test_binarized, y_prob, average="micro")

# Compute macro-average ROC AUC
roc_auc_macro = roc_auc_score(y_test_binarized, y_prob, average="macro")

# Plot ROC curves for each class
plt.figure()
colors = ['blue', 'deeppink', 'cyan', 'green']
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

# Plot micro-average ROC curve
plt.plot(fpr_micro, tpr_micro, color='orangered', linestyle='--', lw=2,
         label='Micro-average ROC curve (area = {0:0.2f})'.format(roc_auc_micro))

# Plot random guessing line
plt.plot([0, 1], [0, 1], 'k--', lw=2)

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC AUC Curves for Multiclass')
plt.legend(loc="lower right")
plt.show()

# Print micro-average and macro-average ROC AUC
print("Micro-average ROC AUC (area = {:.2f})".format(roc_auc_micro))
print("Macro-average ROC AUC (area = {:.2f})".format(roc_auc_macro))

"""### SMOTE

# Data Pre-Processing
"""

# Apply SMOTE only to the training data
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

# Check the class distribution after applying SMOTE
print("Before SMOTE:")
print(y_train.value_counts())
print("\nAfter SMOTE:")
print(pd.Series(y_train_balanced).value_counts())

# Normalize numerical features
numerical_features = ['Age in Month', 'Height']
scaler = StandardScaler()

# Fit the scaler on the training data and transform it
X_train_balanced[numerical_features] = scaler.fit_transform(X_train_balanced[numerical_features])

# Transform the test data using the same scaler
X_test[numerical_features] = scaler.transform(X_test[numerical_features])
X_train_balanced[numerical_features].head()

"""# Model Development"""

# initialize Random Classifier
xg = XGBClassifier()

# define the parameter grid
param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0]
}

# Specify multiple scoring metrics
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score, average='weighted'),
    'recall': make_scorer(recall_score, average='weighted'),
    'f1': make_scorer(f1_score, average='weighted')
}

random_search = RandomizedSearchCV(estimator=xg,
                                   param_distributions=param_dist,
                                   n_iter=50,  # Number of combinations to sample
                                   scoring=scoring,
                                   refit='f1',
                                   cv=kfold,
                                   n_jobs=-1,
                                   verbose=1,
                                   random_state=42)  # For reproducibility

# Fit the model
random_search.fit(X_train_balanced, y_train_balanced)

# Retrieve the best model
best_xgboost_model_smote_tb = random_search.best_estimator_

# Export the best model using joblib
joblib.dump(best_xgboost_model_smote_tb, 'best_xgboost_model_smote_tb.joblib')

# Evaluate the model on the test data
y_pred = best_xgboost_model_smote_tb.predict(X_test)
y_prob = best_xgboost_model_smote_tb.predict_proba(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['NORMAL', 'PENDEK', 'SANGAT PENDEK', 'TINGGI'])
fig, ax = plt.subplots(figsize=(20, 10))
disp.plot(cmap=plt.cm.Blues, ax=ax)
plt.title("Confusion Matrix", fontsize=18)
plt.ylabel("Actual Label", fontsize=15)
plt.xlabel("Predicted Label", fontsize=15)
plt.yticks(fontsize=18)
plt.xticks(fontsize=18)
plt.show()

# Classification evaluation scores (Accuracy, Precision, Recall, F1)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

n_classes = len(np.unique(y_test))

# Binarize the labels for multiclass
y_test_binarized = label_binarize(y_test, classes=np.arange(n_classes))

# Initialize dictionaries for FPR, TPR, and AUC
fpr = dict()
tpr = dict()
roc_auc = dict()

# Compute ROC curve and ROC AUC for each class
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])
    roc_auc[i] = roc_auc_score(y_test_binarized[:, i], y_prob[:, i])

# Compute micro-average ROC curve and ROC AUC
fpr_micro, tpr_micro, _ = roc_curve(y_test_binarized.ravel(), y_prob.ravel())
roc_auc_micro = roc_auc_score(y_test_binarized, y_prob, average="micro")

# Compute macro-average ROC AUC
roc_auc_macro = roc_auc_score(y_test_binarized, y_prob, average="macro")

# Plot ROC curves for each class
plt.figure()
colors = ['blue', 'deeppink', 'cyan', 'green']
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))

# Plot micro-average ROC curve
plt.plot(fpr_micro, tpr_micro, color='orangered', linestyle='--', lw=2,
         label='Micro-average ROC curve (area = {0:0.2f})'.format(roc_auc_micro))

# Plot random guessing line
plt.plot([0, 1], [0, 1], 'k--', lw=2)

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC AUC Curves for Multiclass')
plt.legend(loc="lower right")
plt.show()

# Print micro-average and macro-average ROC AUC
print("Micro-average ROC AUC (area = {:.2f})".format(roc_auc_micro))
print("Macro-average ROC AUC (area = {:.2f})".format(roc_auc_macro))